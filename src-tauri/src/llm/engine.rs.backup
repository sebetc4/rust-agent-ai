/// LLM Engine Module
/// Native llama.cpp integration for standalone all-in-one application

use super::config::LLMConfig;
use crate::context::repository::ConversationRepository;
use crate::context::models::{Conversation, StoredMessage};
use crate::context::database::Database;
use anyhow::{Context, Result};
use llama_cpp_2::{
    llama_backend::LlamaBackend,
    llama_batch::LlamaBatch,
    model::{AddBos, LlamaModel},
    sampling::LlamaSampler,
};
use serde::{Deserialize, Serialize};
use std::num::NonZeroU32;
use std::sync::Arc;
use tokio::sync::Mutex;
use tracing::{info, warn};

/// LLM model response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMResponse {
    pub text: String,
    pub tool_calls: Vec<ToolCall>,
    pub tokens_generated: usize,
    pub done: bool,
}

/// Tool call detected in response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolCall {
    pub name: String,
    pub arguments: serde_json::Value,
}

/// Wrapper for LlamaModel to make it Send + Sync
/// SAFETY: We ensure single-threaded access via Mutex
struct ModelWrapper(LlamaModel);
unsafe impl Send for ModelWrapper {}
unsafe impl Sync for ModelWrapper {}

/// Main LLM engine with native llama.cpp integration
pub struct LLMEngine {
    config: LLMConfig,
    backend: Arc<LlamaBackend>,
    model: Arc<Mutex<Option<ModelWrapper>>>,
    repository: Arc<ConversationRepository>,
    current_session_id: Arc<Mutex<Option<String>>>,
}

impl LLMEngine {
    /// Create a new LLM engine instance
    pub async fn new(config: LLMConfig) -> Result<Self> {
        info!("Initializing native llama.cpp LLM engine...");
        
        // Initialize llama.cpp backend
        let backend = LlamaBackend::init()
            .context("Failed to initialize llama.cpp backend")?;
        
        // Initialize SQLite database
        let db_path = Database::get_default_database_path()
            .context("Failed to get default database path")?;
        let db = Database::new(&db_path).await
            .context("Failed to initialize database")?;
        db.migrate().await
            .context("Failed to run database migrations")?;
        
        let repository = Arc::new(ConversationRepository::new(db.pool().clone()));
        
        Ok(Self {
            config,
            backend: Arc::new(backend),
            model: Arc::new(Mutex::new(None)),
            repository,
            current_session_id: Arc::new(Mutex::new(None)),
        })
    }

    /// Load the LLM model from the configured path
    pub async fn load_model(&self) -> Result<()> {
        let mut model_lock = self.model.lock().await;
        
        // Check if already loaded
        if model_lock.is_some() {
            info!("Model already loaded");
            return Ok(());
        }
        
        // Check if model file exists
        if !self.config.model_path.exists() {
            anyhow::bail!(
                "Model file not found: {}",
                self.config.model_path.display()
            );
        }

        info!("Loading model from: {}", self.config.model_path.display());
        
        // Load the model with default parameters
        let model = LlamaModel::load_from_file(
            &self.backend,
            &self.config.model_path,
            &Default::default(),
        )
        .context("Failed to load GGUF model")?;
        
        info!("Model loaded successfully!");
        info!("Context size: {} tokens", self.config.n_ctx);
        info!("Threads: {}", self.config.n_threads);
        
        *model_lock = Some(ModelWrapper(model));
        
        Ok(())
    }

    /// Check if model is currently loaded
    pub async fn is_loaded(&self) -> bool {
        self.model.lock().await.is_some()
    }

    /// Clear conversation history to start a fresh conversation
    pub async fn clear_conversation(&self) -> Result<String> {
        let session_id = self.create_session("New Conversation").await?;
        info!("Conversation cleared - created new session: {}", session_id);
        Ok(session_id)
    }

    /// Get current conversation history in Qwen3 format in Qwen3 format
    pub async fn get_conversation_history(&self) -> Result<String> {
        let session_id_lock = self.current_session_id.lock().await;
        
        if let Some(session_id) = session_id_lock.as_ref() Result<{
            let messages = self.repository.get_messages(session_id).await?;
            
            let mut history = String::new();
            for msg in messages {
                if !history.is_empty() {
                    history.push('\n');
                }
                history.push_str(&format!("<|im_start|>{}\n{}\n<|im_end|>", msg.role, msg.content));
            }
            
            Ok(history)
        } else {
            Ok(String::new())
        }
    }
    
    /// Create a new conversation session
    pub async fn create_session(&self, title: &str) -> Result<String> {
        let model_name = self.config.model_path
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or("unknown")
            .to_string();
        
        let conversation = self.repository.create_conversation(title, &model_name).await?;
        let mut session_id = self.current_session_id.lock().await;
        *session_id = Some(conversation.id.clone());
        
        info!("Created new session: {} ({})", conversation.title, conversation.id);
        Ok(conversation.id)
    }
    
    /// Load an existing conversation session
    pub async fn load_session(&self, session_id: &str) -> Result<()> {
        // Verify the session exists
        let conversation = self.repository.g>et_conversation(session_id).await?;
        if conversation.is_none() {
        let     anyhow::bail!("Session not found: {}", session_id_ssion_id);
        }
        
        lock = selet mut current_session = self.current_sessiurrent_session_id.lock().await;
        *current_session = Some(session_id.to_string());
        
        info!("Loaded session: {}", session_id);
        Ok(())
    }
    
    /// List all con_id.lock().await;
        
        if let Some(session_id) = session_id_lock.as_ref() {
            let messages = self.repository.get_messages(session sessions
    pub async fn list_id).await?;
            
            let mut sessions(&self, limit: i32, offset: i32) -> Result<Vec<Conversation>> {
        self.repository.list_conversations(limit, offset).await
    }
    
    /// Get the current session ID
    pub async fn current_session_id(&self) -> Option<String> {
        self.current_session_id.lock().await.clone()
    }
    
    /// Delete a conversation session
    pub async fn delete_session(&self, session_id: &str) -> Result<()> {
        self.repository = String::new();
            for msg in messages {
                if !history.delete_conversation(session_id).await?;
        
        // If deleting the current session, clear it
        let mut current = self.current_session_id.is_empty() {
                    history.push('\n');
                }
                history.push_str(&format!("<|im_start|>{}\n{}\n<|it;
        if currenm_end|>", msg.role, msg.as_ref().map(|id| id == session_id).unwrap_or(false) {
            *content));
            }
            
            Ok(history)
        } eurrent = None;
        }
        
        info!("Deleted sessise {
            Ok(String::n: {}", sewssion_id);
        Ok(()))
        }
    }

    /// Generate a response from a prompt

    /// Generate a response from a prompt
    pub async fn generate(&self, prompt: &str) -> Result<LLMResponse> {
        if !self.is_loaded().await {
            anyhow::bail!("No model is loaded. Call load_model() first.");
        }

        info!("Generating response for prompt ({}...)", &prompt[..50.min(prompt.len())]);
        
        // Ensure we have a session
        let mut session_id_lock = self.current_session_id.lock().await;
        if session_id_lock.is_none() {
            drop(session_id_lock);
            let session_id = self.create_session("New Conversation").await?;
            session_id_lock = self.current_session_id.lock().await;
            *session_id_lock = Some(session_id);
        }
        let session_id = session_id_lock.as_ref().unwrap().clone();
        drop(session_id_lock);
        
        // Persist the user message
        let user_msg = StoredMessage::new(session_id.clone(), "user".to_string(), prompt.to_string());
        self.repository.add_message(&user_msg).await?;

        let model_lock = self.model.lock().await;
        let model = &model_lock
            .as_ref()
            .context("Model not loaded despite is_loaded check")?
            .0;
        
        // Load conversation history from database
        let messages = self.repository.get_messages(&session_id).await?;
        let mut history = String::new();
        for msg in &messages {
            if !history.is_empty() {
                history.push('\n');
            }
            history.push_str(&format!("<|im_start|>{}\n{}<|im_end|>", msg.role, msg.content));
        }
        
        // Add assistant start token for generation
        if !history.is_empty() {
            history.push('\n');
        }
        history.push_str("<|im_start|>assistant\n");
        
        // Create context parameters for this generation
        let ctx_params = llama_cpp_2::context::params::LlamaContextParams::default()
            .with_n_ctx(NonZeroU32::new(self.config.n_ctx as u32))
            .with_n_threads(self.config.n_threads as i32);
        
        // Create a new context with the full conversation history
        let mut ctx = model
            .new_context(&self.backend, ctx_params)
            .context("Failed to create context")?;
        
        // Tokenize the FULL conversation history (not just the current prompt)
        let tokens = model
            .str_to_token(&history, AddBos::Always)
            .context("Failed to tokenize conversation history")?;
        
        info!("Conversation history tokenized: {} tokens", tokens.len());
        
        // Create batch for processing
        let mut batch = LlamaBatch::new(self.config.n_ctx as usize, 1);
        
        // Add prompt tokens to batch
        for (i, token) in tokens.iter().enumerate() {
            let is_last = i == tokens.len() - 1;
            batch
                .add(*token, i as i32, &[0], is_last)
                .context("Failed to add token to batch")?;
        }
        
        // Decode the prompt batch
        ctx
            .decode(&mut batch)
            .context("Failed to decode prompt batch")?;
        
        // Generate tokens
        let mut generated_text = String::new();
        let mut tokens_generated = 0;
        let max_tokens = self.config.max_tokens as usize;
        
        // Create sampler chain with configured parameters
        // This uses proper sampling (temperature, top_k, top_p, penalties) instead of greedy sampling
        // Order matters: penalties -> top_k -> top_p -> temperature -> distribution
        // See: https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#sampling
        let mut sampler = LlamaSampler::chain_simple([
            LlamaSampler::penalties(
                64,  // penalty_last_n: consider last 64 tokens for repeat detection
                self.config.repeat_penalty,  // penalty_repeat: from config (default 1.1)
                0.0, // penalty_freq: frequency penalty (0 = disabled for now)
                0.0, // penalty_present: presence penalty (0 = disabled for now)
            ),
            LlamaSampler::top_k(self.config.top_k),  // Keep only top K tokens (default 40)
            LlamaSampler::top_p(self.config.top_p, 1),  // Nucleus sampling with top_p (default 0.9), min_keep=1
            LlamaSampler::temp(self.config.temperature),  // Apply temperature (default 0.7)
            LlamaSampler::dist(0),  // Sample from distribution (seed=0 for deterministic per session)
        ]);
        
        for i in 0..max_tokens {
            // Sample next token using the configured sampler chain
            let next_token = sampler.sample(&ctx, batch.n_tokens() - 1);
            
            // Check for EOS token
            if model.is_eog_token(next_token) {
                info!("Generated {} tokens (EOS reached)", tokens_generated);
                break;
            }
            
            // Decode token to text (skip if it fails, but continue with generation)
            if let Ok(piece) = model.token_to_str(next_token, llama_cpp_2::model::Special::Tokenize) {
                generated_text.push_str(&piece);
                tokens_generated += 1;
            } else {
                warn!("Failed to decode token {}. Continuing generation...", next_token.0);
            }
            
            // Accept the token for repeat penalty tracking
            sampler.accept(next_token);
            
            // Prepare next batch with the new token
            batch.clear();
            let new_pos = tokens.len() as i32 + i as i32;
            batch
                .add(next_token, new_pos, &[0], true)
                .context("Failed to add generated token to batch")?;
            
            // Decode the new token
            ctx
                .decode(&mut batch)
                .context("Failed to decode generated token")?;
        }
        
        info!("Generated {} tokens", tokens_generated);
        
        // Persist the assistant's response
        let assistant_msg = StoredMessage::new(
            session_id.clone(),
            "assistant".to_string(),
            generated_text.trim().to_string(),
        ).with_tokens(tokens_generated as i32);
        self.repository.add_message(&assistant_msg).await?;
        
        Ok(LLMResponse {
            text: generated_text.trim().to_string(),
            tool_calls: Self::parse_tool_calls(&generated_text),
            tokens_generated,
            done: true,
        })
    }

    /// Generate a streaming response (callback receives chunks)
    pub async fn generate_stream<F>(
        &self,
        prompt: &str,
        mut callback: F,
    ) -> Result<LLMResponse>
    where
        F: FnMut(String) -> Result<()>,
    {
        if !self.is_loaded().await {
            anyhow::bail!("No model is loaded. Call load_model() first.");
        }

        info!("Generating streaming response for prompt ({}...)", &prompt[..50.min(prompt.len())]);

        let model_lock = self.model.lock().await;
        let model = &model_lock
            .as_ref()
            .context("Model not loaded despite is_loaded check")?
            .0;
        
        // Create context for this generation
        let ctx_params = llama_cpp_2::context::params::LlamaContextParams::default()
            .with_n_ctx(NonZeroU32::new(self.config.n_ctx as u32))
            .with_n_threads(self.config.n_threads as i32);
        
        let mut ctx = model.new_context(&self.backend, ctx_params)?;
        
        // Tokenize prompt
        let tokens = model
            .str_to_token(prompt, AddBos::Always)
            .context("Failed to tokenize prompt")?;
        
        let mut batch = LlamaBatch::new(self.config.n_ctx as usize, 1);
        
        // Process prompt
        for (i, token) in tokens.iter().enumerate() {
            batch
                .add(*token, i as i32, &[0], i == tokens.len() - 1)
                .context("Failed to add token")?;
        }
        
        ctx.decode(&mut batch)?;
        
        // Generate with streaming
        let mut generated_text = String::new();
        let mut tokens_generated = 0;
        let max_tokens = self.config.max_tokens as usize;
        
        for i in 0..max_tokens {
            let candidates = ctx.candidates_ith(batch.n_tokens() - 1);
            let next_token = candidates
                .into_iter()
                .max_by(|a, b| a.logit().partial_cmp(&b.logit()).unwrap())
                .map(|d| d.id())
                .context("No candidates")?;
            
            if model.is_eog_token(next_token) {
                break;
            }
            
            let piece = model.token_to_str(next_token, llama_cpp_2::model::Special::Tokenize)?;
            
            // Stream the chunk
            callback(piece.clone())?;
            
            generated_text.push_str(&piece);
            tokens_generated += 1;
            
            batch.clear();
            batch.add(next_token, tokens.len() as i32 + i as i32, &[0], true)?;
            ctx.decode(&mut batch)?;
        }
        
        let tool_calls = Self::parse_tool_calls(&generated_text);
        
        Ok(LLMResponse {
            text: generated_text,
            tool_calls,
            tokens_generated,
            done: true,
        })
    }

    /// Parse tool calls from response text (placeholder for future implementation)
    fn parse_tool_calls(_text: &str) -> Vec<ToolCall> {
        // TODO: Implement tool call detection based on JSON format
        vec![]
    }

    /// Unload model from memory
    pub async fn unload_model(&self) -> Result<()> {
        info!("Unloading model");
        let mut model_lock = self.model.lock().await;
        *model_lock = None;
        info!("Model unloaded successfully");
        Ok(())
    }

    /// Get current configuration
    pub fn config(&self) -> &LLMConfig {
        &self.config
    }

    /// Update configuration (requires reload)
    pub fn set_config(&mut self, config: LLMConfig) {
        warn!("Configuration changed. Model must be reloaded.");
        self.config = config;
    }
}

impl Drop for LLMEngine {
    fn drop(&mut self) {
        info!("LLMEngine dropping - cleanup will occur automatically");
    }
}
